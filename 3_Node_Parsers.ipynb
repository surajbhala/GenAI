{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surajbhala/GenAI/blob/main/3_Node_Parsers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e244c591-83f5-4e13-9c3d-1546d2914eaf",
      "metadata": {
        "id": "e244c591-83f5-4e13-9c3d-1546d2914eaf"
      },
      "source": [
        "# Node Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ddbc1d32-6a75-45cf-ac92-af3f619a0745",
      "metadata": {
        "id": "ddbc1d32-6a75-45cf-ac92-af3f619a0745"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ebe30d58-2f66-4582-9414-700f85d53ad8",
      "metadata": {
        "id": "ebe30d58-2f66-4582-9414-700f85d53ad8"
      },
      "outputs": [],
      "source": [
        "# # Load environment variables from the .env file\n",
        "# load_dotenv('D:/.env')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8368c993-b08d-4caf-8794-37d2d67bf017",
      "metadata": {
        "id": "8368c993-b08d-4caf-8794-37d2d67bf017"
      },
      "outputs": [],
      "source": [
        "# Retrieve the OpenAI API key from environment variables\n",
        "#OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
        "#OPENAI_API_KEY = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9391a612-08e3-4a93-834f-1e447e67af24",
      "metadata": {
        "id": "9391a612-08e3-4a93-834f-1e447e67af24"
      },
      "source": [
        "Download the required packages by executing the below commands in either Anaconda Prompt (in Windows) or Terminal (in Linux or Mac OS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c751d55-6e87-4249-a385-d84b7c66c3b5",
      "metadata": {
        "id": "8c751d55-6e87-4249-a385-d84b7c66c3b5"
      },
      "source": [
        "pip install tree-sitter_languages tree-sitter==0.21.3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fda08fe6-6696-4db4-9f41-854f667fc496",
      "metadata": {
        "id": "fda08fe6-6696-4db4-9f41-854f667fc496"
      },
      "source": [
        "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcd65074-3be3-46d0-b618-11f5248ec460",
      "metadata": {
        "id": "bcd65074-3be3-46d0-b618-11f5248ec460"
      },
      "source": [
        "# Creating Nodes Manually"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install llama_index"
      ],
      "metadata": {
        "id": "TOaSE7UVu66q"
      },
      "id": "TOaSE7UVu66q",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "807c80cd-7942-4ef6-ade9-9b988853351c",
      "metadata": {
        "id": "807c80cd-7942-4ef6-ade9-9b988853351c"
      },
      "outputs": [],
      "source": [
        "## Create a document and split it into multiple chunks\n",
        "## IMport document class which will be helpful in creating a document object\n",
        "## Import Text Node to create nodes in the document\n",
        "from llama_index.core import Document\n",
        "from llama_index.core.schema import TextNode\n",
        "\n",
        "## Sample string\n",
        "doc = Document(text=\"This is a sample document text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5a5ce24e-89cd-4e34-9456-1f96700402d1",
      "metadata": {
        "id": "5a5ce24e-89cd-4e34-9456-1f96700402d1",
        "outputId": "a2f3b2d6-7d90-467d-cead-6844a97a3bfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'b7d3016b-39dd-4cbd-9e7c-989bad03fa14'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "## Each document class gets a document ID in llamaindex\n",
        "doc.doc_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "490514eb-8f60-49dc-8035-0da2f28a3a68",
      "metadata": {
        "id": "490514eb-8f60-49dc-8035-0da2f28a3a68",
        "outputId": "c8be683b-be9e-49f4-c535-1ab3c6215943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='b7d3016b-39dd-4cbd-9e7c-989bad03fa14', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='This is a sample document text', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\n",
        "doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "77fce153-a341-426f-ba57-acde672312e8",
      "metadata": {
        "id": "77fce153-a341-426f-ba57-acde672312e8",
        "outputId": "c7b5324f-4e5f-4b36-9a20-d597a41114a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id_': 'b7d3016b-39dd-4cbd-9e7c-989bad03fa14',\n",
              " 'embedding': None,\n",
              " 'metadata': {},\n",
              " 'excluded_embed_metadata_keys': [],\n",
              " 'excluded_llm_metadata_keys': [],\n",
              " 'relationships': {},\n",
              " 'metadata_template': '{key}: {value}',\n",
              " 'metadata_separator': '\\n',\n",
              " 'text_resource': {'embeddings': None,\n",
              "  'text': 'This is a sample document text',\n",
              "  'path': None,\n",
              "  'url': None,\n",
              "  'mimetype': None},\n",
              " 'image_resource': None,\n",
              " 'audio_resource': None,\n",
              " 'video_resource': None,\n",
              " 'text_template': '{metadata_str}\\n\\n{content}',\n",
              " 'class_name': 'Document',\n",
              " 'text': 'This is a sample document text'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "## Looking at what the document object contains\n",
        "doc.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1a17e562-170b-48f6-87b9-f80fe5427403",
      "metadata": {
        "id": "1a17e562-170b-48f6-87b9-f80fe5427403"
      },
      "outputs": [],
      "source": [
        "## Metadata can be specified after the document has been created\n",
        "doc.metadata = {\"report_name\": \"Competition Analysis Report May 2024\",\\\n",
        "                     \"department\": \"Marketing\",\\\n",
        "                     \"author\": \"Prashant\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a044d334-ed63-4e95-b5bb-6ce94e333206",
      "metadata": {
        "id": "a044d334-ed63-4e95-b5bb-6ce94e333206",
        "outputId": "b15ee5d5-cfdd-4292-cf58-805bdb07b65b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id_': 'b7d3016b-39dd-4cbd-9e7c-989bad03fa14',\n",
              " 'embedding': None,\n",
              " 'metadata': {'report_name': 'Competition Analysis Report May 2024',\n",
              "  'department': 'Marketing',\n",
              "  'author': 'Prashant'},\n",
              " 'excluded_embed_metadata_keys': [],\n",
              " 'excluded_llm_metadata_keys': [],\n",
              " 'relationships': {},\n",
              " 'metadata_template': '{key}: {value}',\n",
              " 'metadata_separator': '\\n',\n",
              " 'text_resource': {'embeddings': None,\n",
              "  'text': 'This is a sample document text',\n",
              "  'path': None,\n",
              "  'url': None,\n",
              "  'mimetype': None},\n",
              " 'image_resource': None,\n",
              " 'audio_resource': None,\n",
              " 'video_resource': None,\n",
              " 'text_template': '{metadata_str}\\n\\n{content}',\n",
              " 'class_name': 'Document',\n",
              " 'text': 'This is a sample document text'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "doc.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "68f4d32c-985d-4ce8-a3b9-abb955ff96cd",
      "metadata": {
        "id": "68f4d32c-985d-4ce8-a3b9-abb955ff96cd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4d9d825c-0646-47d9-9987-fbf8073738b1",
      "metadata": {
        "id": "4d9d825c-0646-47d9-9987-fbf8073738b1",
        "outputId": "3f77892b-b05b-44e9-d6d2-6057601fb08d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{<NodeRelationship.NEXT: '3'>: '28b0005a-011d-4add-afce-80307ab73be2'}\n",
            "{<NodeRelationship.PREVIOUS: '2'>: '2c8b8f20-d090-4a55-be4f-f9e679871b28'}\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Document\n",
        "from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
        "\n",
        "doc = Document(text=\"First sentence. Second Sentence\")\n",
        "\n",
        "#Creating nodes manually\n",
        "n1 = TextNode(text=\"First sentence\", node_id=doc.doc_id)\n",
        "n2 = TextNode(text=\"Second sentence\", node_id=doc.doc_id)\n",
        "\n",
        "## Creating relationships between nodes\n",
        "n1.relationships[NodeRelationship.NEXT] = n2.node_id\n",
        "n2.relationships[NodeRelationship.PREVIOUS] = n1.node_id\n",
        "\n",
        "print(n1.relationships)\n",
        "print(n2.relationships)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n1.dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptJzn3QSwqBP",
        "outputId": "c6465e34-c226-4e90-96c0-403f4bbcdf0e"
      },
      "id": "ptJzn3QSwqBP",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/llama_index/core/schema.py:116: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected `RelatedNodeInfo` - serialized value may not be as expected [input_value='28b0005a-011d-4add-afce-80307ab73be2', input_type=str])\n",
            "  PydanticSerializationUnexpectedValue(Expected `list[definition-ref]` - serialized value may not be as expected [input_value='28b0005a-011d-4add-afce-80307ab73be2', input_type=str])\n",
            "  data = handler(self)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id_': '2c8b8f20-d090-4a55-be4f-f9e679871b28',\n",
              " 'embedding': None,\n",
              " 'metadata': {},\n",
              " 'excluded_embed_metadata_keys': [],\n",
              " 'excluded_llm_metadata_keys': [],\n",
              " 'relationships': {'3': '28b0005a-011d-4add-afce-80307ab73be2'},\n",
              " 'metadata_template': '{key}: {value}',\n",
              " 'metadata_separator': '\\n',\n",
              " 'text': 'First sentence',\n",
              " 'mimetype': 'text/plain',\n",
              " 'start_char_idx': None,\n",
              " 'end_char_idx': None,\n",
              " 'metadata_seperator': '\\n',\n",
              " 'text_template': '{metadata_str}\\n\\n{content}',\n",
              " 'class_name': 'TextNode'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n2.dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVPbNuiVwqGD",
        "outputId": "04417ed5-a6e9-41aa-a65a-753d5cf91b48"
      },
      "id": "qVPbNuiVwqGD",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/llama_index/core/schema.py:116: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected `RelatedNodeInfo` - serialized value may not be as expected [input_value='2c8b8f20-d090-4a55-be4f-f9e679871b28', input_type=str])\n",
            "  PydanticSerializationUnexpectedValue(Expected `list[definition-ref]` - serialized value may not be as expected [input_value='2c8b8f20-d090-4a55-be4f-f9e679871b28', input_type=str])\n",
            "  data = handler(self)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id_': '28b0005a-011d-4add-afce-80307ab73be2',\n",
              " 'embedding': None,\n",
              " 'metadata': {},\n",
              " 'excluded_embed_metadata_keys': [],\n",
              " 'excluded_llm_metadata_keys': [],\n",
              " 'relationships': {'2': '2c8b8f20-d090-4a55-be4f-f9e679871b28'},\n",
              " 'metadata_template': '{key}: {value}',\n",
              " 'metadata_separator': '\\n',\n",
              " 'text': 'Second sentence',\n",
              " 'mimetype': 'text/plain',\n",
              " 'start_char_idx': None,\n",
              " 'end_char_idx': None,\n",
              " 'metadata_seperator': '\\n',\n",
              " 'text_template': '{metadata_str}\\n\\n{content}',\n",
              " 'class_name': 'TextNode'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n1.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hdJB7zeRw0AX",
        "outputId": "1237714c-6f4b-4885-d77f-176a9f8cffe9"
      },
      "id": "hdJB7zeRw0AX",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First sentence'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac9ed8fb-e7fc-46f8-a6e7-255f2407a6ff",
      "metadata": {
        "id": "ac9ed8fb-e7fc-46f8-a6e7-255f2407a6ff"
      },
      "source": [
        "In this example, we‚Äôve manually created two Nodes and defined a previous or next relationship between them. The relationship tracks the order of Nodes within the original Document. This code tells LlamaIndex that the two Nodes belong to the initial Document and they also come in a particular order."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e41a5996-6589-4d80-9cae-9a2307b12de4",
      "metadata": {
        "id": "e41a5996-6589-4d80-9cae-9a2307b12de4"
      },
      "source": [
        "# File-Based Node Parsers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5886533-a497-4c83-acbd-5afab4ee9af4",
      "metadata": {
        "id": "e5886533-a497-4c83-acbd-5afab4ee9af4"
      },
      "source": [
        "## HTML Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c4e58c5a-3faf-4e37-9735-35092929c2b4",
      "metadata": {
        "id": "c4e58c5a-3faf-4e37-9735-35092929c2b4"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4e40ce4e-d3dd-4598-ad57-39e38cc681ba",
      "metadata": {
        "id": "4e40ce4e-d3dd-4598-ad57-39e38cc681ba"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "31e6d3b8-c86d-4e54-9f86-d32f9e7c8356",
      "metadata": {
        "id": "31e6d3b8-c86d-4e54-9f86-d32f9e7c8356"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import HTMLNodeParser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2984421-72f7-49f0-ad9b-c77842899c5b",
      "metadata": {
        "id": "c2984421-72f7-49f0-ad9b-c77842899c5b"
      },
      "source": [
        "- This parser uses Beautiful Soup to parse HTML files and convert them into nodes based on selected HTML tags.\n",
        "- This parser simplifies the HTML file by extracting text from standard text elements and merging adjacent nodes of the same type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "19e2948f-da3c-4446-8f6d-bdb5d65e80d2",
      "metadata": {
        "id": "19e2948f-da3c-4446-8f6d-bdb5d65e80d2",
        "outputId": "bf75211d-fc61-453b-bd9c-9c36b1f0a239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ],
      "source": [
        "# URL of the website to fetch HTML from\n",
        "url = \"https://docs.llamaindex.ai/en/stable/\"\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "23a2c532-e6f7-4380-ad6e-a3923175c5fd",
      "metadata": {
        "id": "23a2c532-e6f7-4380-ad6e-a3923175c5fd"
      },
      "outputs": [],
      "source": [
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Extract the HTML content from the response\n",
        "    html_doc = response.text\n",
        "\n",
        "    # Create a Document object with the HTML content\n",
        "    document = Document(id_=url, text=html_doc)\n",
        "\n",
        "    # Initialize the HTMLNodeParser with optional list of tags\n",
        "    parser = HTMLNodeParser()\n",
        "\n",
        "    # Parse nodes from the HTML document\n",
        "    nodes = parser.get_nodes_from_documents([document])\n",
        "\n",
        "else:\n",
        "    # Print an error message if the request was unsuccessful\n",
        "    print(\"Failed to fetch HTML content:\", response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fad2f587-04fe-45a7-a421-d02f74da1149",
      "metadata": {
        "id": "fad2f587-04fe-45a7-a421-d02f74da1149",
        "outputId": "797ebcd8-d8ea-40fe-d842-61994375bde2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "20d94519-3c74-4885-b3e2-b1a9da263112",
      "metadata": {
        "id": "20d94519-3c74-4885-b3e2-b1a9da263112",
        "outputId": "8f859b46-8d06-4fb9-d6b2-2e327119b82c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id_': 'affc6af9-c433-46fe-96fb-64c7705a5faf',\n",
              " 'embedding': None,\n",
              " 'metadata': {'tag': 'h1'},\n",
              " 'excluded_embed_metadata_keys': [],\n",
              " 'excluded_llm_metadata_keys': [],\n",
              " 'relationships': {'1': {'node_id': 'https://docs.llamaindex.ai/en/stable/',\n",
              "   'node_type': '4',\n",
              "   'metadata': {},\n",
              "   'hash': 'd76dff10cc979e128ce5b50ec66fa202adc916bc7b61b50ac70a022b4a9dadfe',\n",
              "   'class_name': 'RelatedNodeInfo'},\n",
              "  '2': {'node_id': 'eb733742-a8c5-4d00-b6cb-c2453aeccf9f',\n",
              "   'node_type': '1',\n",
              "   'metadata': {'tag': 'li'},\n",
              "   'hash': '969083e7f3c54fb307a60a98d3a8eb8fbadb5197fb6d8054020d3a3fd01ef857',\n",
              "   'class_name': 'RelatedNodeInfo'},\n",
              "  '3': {'node_id': '3bfec2d1-c57d-4908-9ef2-6830c675e379',\n",
              "   'node_type': '1',\n",
              "   'metadata': {'tag': 'p'},\n",
              "   'hash': '627729ad274386073ab3208436c5cb22815cbb175995ea0e6a581d7c5b1d3cf3',\n",
              "   'class_name': 'RelatedNodeInfo'}},\n",
              " 'metadata_template': '{key}: {value}',\n",
              " 'metadata_separator': '\\n',\n",
              " 'text': 'Welcome to LlamaIndex ü¶ô !\\n#',\n",
              " 'mimetype': 'text/plain',\n",
              " 'start_char_idx': None,\n",
              " 'end_char_idx': None,\n",
              " 'metadata_seperator': '\\n',\n",
              " 'text_template': '{metadata_str}\\n\\n{content}',\n",
              " 'class_name': 'TextNode'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "nodes[1].to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32903aaf-b195-4fe3-af14-96dced18afab",
      "metadata": {
        "id": "32903aaf-b195-4fe3-af14-96dced18afab"
      },
      "source": [
        "You have the option to customize the HTML tags from where you want to retrieve content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a83b43e5-a063-4b69-b3a8-54296a2727f9",
      "metadata": {
        "id": "a83b43e5-a063-4b69-b3a8-54296a2727f9"
      },
      "outputs": [],
      "source": [
        "my_tags = [\"p\", \"span\"]\n",
        "html_parser = HTMLNodeParser(tags=my_tags)\n",
        "nodes = html_parser.get_nodes_from_documents([document])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "9648f195-97f4-4df7-a82f-cdbb8f4c57f1",
      "metadata": {
        "scrolled": true,
        "id": "9648f195-97f4-4df7-a82f-cdbb8f4c57f1",
        "outputId": "fb0a3c49-7707-48fa-9c7d-9f8e46b38089",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<span> elements:\n",
            "LlamaIndex\n",
            "LlamaIndex\n",
            "Home\n",
            "\n",
            "\n",
            "High-Level Concepts\n",
            "Installation and Setup\n",
            "How to read these docs\n",
            "Starter Examples\n",
            "\n",
            "Discover LlamaIndex Video Series\n",
            "Frequently Asked Questions (FAQ)\n",
            "Starter Tools\n",
            "\n",
            "Learn\n",
            "\n",
            "Use Cases\n",
            "\n",
            "Examples\n",
            "\n",
            "Component Guides\n",
            "\n",
            "Advanced Topics\n",
            "\n",
            "API Reference\n",
            "\n",
            "Open-Source Community\n",
            "\n",
            "Workflows\n",
            "\n",
            "LlamaCloud\n",
            "\n",
            "Introduction\n",
            "What are agents?\n",
            "What are workflows?\n",
            "What is context augmentation?\n",
            "LlamaIndex is the framework for Context-Augmented LLM Applications\n",
            "Use cases\n",
            "üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Who is LlamaIndex for?\n",
            "Getting Started\n",
            "30 second quickstart\n",
            "LlamaCloud\n",
            "Community\n",
            "Getting the library\n",
            "Contributing\n",
            "LlamaIndex Ecosystem\n",
            "Introduction\n",
            "Use cases\n",
            "Getting started\n",
            "LlamaCloud\n",
            "Community\n",
            "Related projects\n",
            "\n",
            "from\n",
            "llama_index.core\n",
            "import\n",
            "VectorStoreIndex\n",
            ",\n",
            "SimpleDirectoryReader\n",
            "documents\n",
            "=\n",
            "SimpleDirectoryReader\n",
            "(\n",
            "\"data\"\n",
            ")\n",
            ".\n",
            "load_data\n",
            "()\n",
            "index\n",
            "=\n",
            "VectorStoreIndex\n",
            ".\n",
            "from_documents\n",
            "(\n",
            "documents\n",
            ")\n",
            "query_engine\n",
            "=\n",
            "index\n",
            ".\n",
            "as_query_engine\n",
            "()\n",
            "response\n",
            "=\n",
            "query_engine\n",
            ".\n",
            "query\n",
            "(\n",
            "\"Some question about the data should go here\"\n",
            ")\n",
            "print\n",
            "(\n",
            "response\n",
            ")\n",
            "Next\n"
          ]
        }
      ],
      "source": [
        "print('<span> elements:')\n",
        "for node in nodes:\n",
        "    if node.metadata['tag']=='span':\n",
        "        print(node.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5b143c7d-c3f8-45a6-a7d9-6e5a0382b17c",
      "metadata": {
        "id": "5b143c7d-c3f8-45a6-a7d9-6e5a0382b17c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6f243d85-2927-4dea-929c-e349408b32c4",
      "metadata": {
        "id": "6f243d85-2927-4dea-929c-e349408b32c4",
        "outputId": "4ec2ddce-f4d6-4942-85ba-87f1e40f7448",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<p> elements:\n",
            "LlamaIndex is the leading framework for building LLM-powered agents over your data with\n",
            "LLMs\n",
            "and\n",
            "workflows\n",
            ".\n",
            "What is context augmentation? What are agents and workflows? How does LlamaIndex help build them?\n",
            "What kind of apps can you build with LlamaIndex? Who should use it?\n",
            "Get started in Python or TypeScript in just 5 lines of code!\n",
            "Managed services for LlamaIndex including\n",
            "LlamaParse\n",
            ", the world's best document parser.\n",
            "Get help and meet collaborators on Discord, Twitter, LinkedIn, and learn how to contribute to the project.\n",
            "Check out our library of connectors, readers, and other integrations at\n",
            "LlamaHub\n",
            "as well as demos and starter apps like\n",
            "create-llama\n",
            ".\n",
            "Agents\n",
            "are LLM-powered knowledge assistants that use tools to perform tasks like research, data extraction, and more. Agents range from simple question-answering to being able to sense, decide and take actions in order to complete tasks.\n",
            "LlamaIndex provides a framework for building agents including the ability to use RAG pipelines as one of many tools to complete a task.\n",
            "Workflows\n",
            "are multi-step processes that combine one or more agents, data connectors, and other tools to complete a task. They are event-driven software that allows you to combine RAG data sources and multiple agents to create a complex application that can perform a wide variety of tasks with reflection, error-correction, and other hallmarks of advanced LLM applications. You can then\n",
            "deploy these agentic workflows\n",
            "as production microservices.\n",
            "LLMs offer a natural language interface between humans and data. LLMs come pre-trained on huge amounts of publicly available data, but they are not trained on\n",
            "your\n",
            "data. Your data may be private or specific to the problem you're trying to solve. It's behind APIs, in SQL databases, or trapped in PDFs and slide decks.\n",
            "Context augmentation makes your data available to the LLM to solve the problem at hand. LlamaIndex provides the tools to build any of context-augmentation use case, from prototype to production. Our tools allow you to ingest, parse, index and process your data and quickly implement complex query workflows combining data access with LLM prompting.\n",
            "The most popular example of context-augmentation is\n",
            "Retrieval-Augmented Generation or RAG\n",
            ", which combines context with LLMs at inference time.\n",
            "LlamaIndex imposes no restriction on how you use LLMs. You can use LLMs as auto-complete, chatbots, agents, and more. It just makes using them easier. We provide tools like:\n",
            "Some popular use cases for LlamaIndex and context augmentation in general include:\n",
            "Check out our\n",
            "use cases\n",
            "documentation for more examples and links to tutorials.\n",
            "LlamaIndex provides tools for beginners, advanced users, and everyone in between.\n",
            "Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.\n",
            "For more complex applications, our lower-level APIs allow advanced users to customize and extend any module -- data connectors, indices, retrievers, query engines, and reranking modules -- to fit their needs.\n",
            "LlamaIndex is available in Python (these docs) and\n",
            "Typescript\n",
            ". If you're not sure where to start, we recommend reading\n",
            "how to read these docs\n",
            "which will point you to the right place based on your experience level.\n",
            "Set an environment variable called\n",
            "OPENAI_API_KEY\n",
            "with an\n",
            "OpenAI API key\n",
            ". Install the Python library:\n",
            "Put some documents in a folder called\n",
            "data\n",
            ", then ask questions about them with our famous 5-line starter:\n",
            "If any part of this trips you up, don't worry! Check out our more comprehensive starter tutorials using\n",
            "remote APIs like OpenAI\n",
            "or\n",
            "any model that runs on your laptop\n",
            ".\n",
            "If you're an enterprise developer, check out\n",
            "LlamaCloud\n",
            ". It is an end-to-end managed service for document parsing, extraction, indexing, and retrieval - allowing you to get production-quality data for your AI agent. You can\n",
            "sign up\n",
            "and get 10,000 free credits per month, sign up for one of our\n",
            "plans\n",
            ", or\n",
            "come talk to us\n",
            "if you're interested in an enterprise solution. We offer both SaaS and self-hosted plans.\n",
            "You can also check out the\n",
            "LlamaCloud documentation\n",
            "for more details.\n",
            "Need help? Have a feature suggestion? Join the LlamaIndex community:\n",
            "We are open-source and always welcome contributions to the project! Check out our\n",
            "contributing guide\n",
            "for full details on how to extend the core library or add an integration to a third party like an LLM, a vector store, an agent tool and more.\n",
            "There's more to the LlamaIndex universe! Check out some of our other projects:\n"
          ]
        }
      ],
      "source": [
        "print('<p> elements:')\n",
        "for node in nodes:\n",
        "    if node.metadata['tag']=='p':\n",
        "        print(node.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "cd56da49-8a15-45a3-9513-c60c2db292d2",
      "metadata": {
        "id": "cd56da49-8a15-45a3-9513-c60c2db292d2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d348cc80-e3c3-456b-8b06-9ba43ec31f87",
      "metadata": {
        "id": "d348cc80-e3c3-456b-8b06-9ba43ec31f87"
      },
      "source": [
        "## Simple File Node Parser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04f98466-a1a6-4b0d-a0f4-b47703ea7789",
      "metadata": {
        "id": "04f98466-a1a6-4b0d-a0f4-b47703ea7789"
      },
      "source": [
        "- This one automatically decides which of the following three node parsers should be used based on file types.\n",
        "- It can automatically handle these file formats and transform them into nodes, simplifying the process of interacting with various types of content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "821a2bab-382e-4f0d-bfa4-60b300d3f9ed",
      "metadata": {
        "id": "821a2bab-382e-4f0d-bfa4-60b300d3f9ed"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SimpleFileNodeParser\n",
        "from llama_index.readers.file import FlatReader\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95d81b48-1c62-42ce-89da-20f5ece5a1f4",
      "metadata": {
        "id": "95d81b48-1c62-42ce-89da-20f5ece5a1f4"
      },
      "source": [
        "Works for json, markdown, and html files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c7ae6e7a-bfd6-40e1-84ba-02aa9eff56ff",
      "metadata": {
        "id": "c7ae6e7a-bfd6-40e1-84ba-02aa9eff56ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "b9e8c108-884e-4384-e691-cfc75ff7681c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/data/README.md'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1504711201.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/README.md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/readers/file/flat/base.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, file, extra_info, fs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m\"\"\"Parse file into string.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_fs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"filename\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"extension\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"autocommit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             f = self._open(\n\u001b[0m\u001b[1;32m   1311\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_mkdir\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLocalFileOpener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtouch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocommit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                     \u001b[0mcompress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/README.md'"
          ]
        }
      ],
      "source": [
        "documents = FlatReader().load_data(Path('data/README.md'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf7c5dbf-ece1-4ea1-80c0-c60358fe9dbf",
      "metadata": {
        "id": "cf7c5dbf-ece1-4ea1-80c0-c60358fe9dbf"
      },
      "source": [
        "You can simply rely on `FlatReader` to load the file into your `Document` object; `SimpleFileNodeParser` will know what to do from there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16da76ad-efce-4637-833a-619a060c524d",
      "metadata": {
        "id": "16da76ad-efce-4637-833a-619a060c524d"
      },
      "outputs": [],
      "source": [
        "parser = SimpleFileNodeParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f77cd6c-8fbe-4519-8f2a-5ecc2629d9b9",
      "metadata": {
        "id": "9f77cd6c-8fbe-4519-8f2a-5ecc2629d9b9"
      },
      "outputs": [],
      "source": [
        "nodes = parser.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a0772f-c40b-4b7b-8e4c-ab4ebbbe487b",
      "metadata": {
        "id": "e2a0772f-c40b-4b7b-8e4c-ab4ebbbe487b"
      },
      "outputs": [],
      "source": [
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18104bd0-9b08-47bc-b972-76266ad33b84",
      "metadata": {
        "id": "18104bd0-9b08-47bc-b972-76266ad33b84"
      },
      "outputs": [],
      "source": [
        "nodes[5].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd13efe-5c26-4233-bbdf-747834ff0410",
      "metadata": {
        "id": "2cd13efe-5c26-4233-bbdf-747834ff0410"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416d2460-448f-45f2-a361-d1bfead62711",
      "metadata": {
        "id": "416d2460-448f-45f2-a361-d1bfead62711"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0e90283f-ae50-46ba-ab13-8e9a0ef2d3d7",
      "metadata": {
        "id": "0e90283f-ae50-46ba-ab13-8e9a0ef2d3d7"
      },
      "source": [
        "# Text Splitters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11aae609-216a-4b47-a2bd-bf86fc849450",
      "metadata": {
        "id": "11aae609-216a-4b47-a2bd-bf86fc849450"
      },
      "source": [
        "## Code splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "d7550d83-12f0-4ce9-a908-97300a6a905f",
      "metadata": {
        "id": "d7550d83-12f0-4ce9-a908-97300a6a905f"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b1fcb4c1-7f84-4f70-95ab-2bed13e20918",
      "metadata": {
        "id": "b1fcb4c1-7f84-4f70-95ab-2bed13e20918"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import CodeSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f448e3c7-ceb1-4d35-931e-2817168bbf43",
      "metadata": {
        "id": "f448e3c7-ceb1-4d35-931e-2817168bbf43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "b847b3c6-26ab-4421-d745-7ce43df52040"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "File data/settings.py does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3737399142.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleDirectoryReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data/settings.py'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/readers/file/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dir, input_files, exclude, exclude_hidden, exclude_empty, errors, recursive, encoding, filename_as_id, required_exts, file_extractor, num_files_limit, file_metadata, raise_on_error, fs)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File {path} does not exist.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: File data/settings.py does not exist."
          ]
        }
      ],
      "source": [
        "documents = SimpleDirectoryReader(input_files=['data/settings.py']).load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "2ac99e7f-92c8-4443-b75c-f844598d8eb9",
      "metadata": {
        "id": "2ac99e7f-92c8-4443-b75c-f844598d8eb9",
        "outputId": "f90235d8-80b0-4ca6-82df-a4c764869f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tree_sitter'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2498918507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m splitter = CodeSplitter(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"python\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mchunk_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# lines per chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mchunk_lines_overlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# lines overlap between chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# max chars per chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/node_parser/text/code.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, chunk_lines, chunk_lines_overlap, max_chars, parser, callback_manager, include_metadata, include_prev_next_rel, id_func)\u001b[0m\n\u001b[1;32m     56\u001b[0m     ) -> None:\n\u001b[1;32m     57\u001b[0m         \u001b[0;34m\"\"\"Initialize a CodeSplitter.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtree_sitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParser\u001b[0m  \u001b[0;31m# pants: no-infer-dep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mcallback_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback_manager\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallbackManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tree_sitter'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "splitter = CodeSplitter(\n",
        "    language=\"python\",\n",
        "    chunk_lines=40,  # lines per chunk\n",
        "    chunk_lines_overlap=15,  # lines overlap between chunks\n",
        "    max_chars=1500,  # max chars per chunk\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b6e500e-2f87-47e2-975f-43ea8b8fb60e",
      "metadata": {
        "id": "4b6e500e-2f87-47e2-975f-43ea8b8fb60e"
      },
      "source": [
        "- `language`: This specifies the language of the code\n",
        "- `chunk_lines`: This defines the number of lines per chunk\n",
        "- `chunk_lines_overlap`: This defines the lines overlap between chunks\n",
        "- `max_chars`: This defines the maximum characters per chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8048410-b458-4f93-a7e3-4c407ac18789",
      "metadata": {
        "id": "e8048410-b458-4f93-a7e3-4c407ac18789"
      },
      "outputs": [],
      "source": [
        "nodes = splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83bc43e4-41a2-4a0d-a79c-eea289d7f450",
      "metadata": {
        "id": "83bc43e4-41a2-4a0d-a79c-eea289d7f450"
      },
      "outputs": [],
      "source": [
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ffc132-bf74-4847-95a1-b04860de0959",
      "metadata": {
        "id": "d8ffc132-bf74-4847-95a1-b04860de0959"
      },
      "outputs": [],
      "source": [
        "nodes[0].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "693d87a4-ed8d-4c49-94a8-55616d25f52e",
      "metadata": {
        "id": "693d87a4-ed8d-4c49-94a8-55616d25f52e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10ca3505-3436-45af-a8fe-37ba5eb8ed21",
      "metadata": {
        "id": "10ca3505-3436-45af-a8fe-37ba5eb8ed21"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "656ab548-22d9-4ff6-8122-ff7dfd70603c",
      "metadata": {
        "id": "656ab548-22d9-4ff6-8122-ff7dfd70603c"
      },
      "source": [
        "## Sentense Splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "201ba503-f066-4d82-a967-068648cbd54c",
      "metadata": {
        "id": "201ba503-f066-4d82-a967-068648cbd54c"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "b2a7ac98-8929-460c-9852-ebe0b5b3f9b4",
      "metadata": {
        "id": "b2a7ac98-8929-460c-9852-ebe0b5b3f9b4"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(input_files=['data/paul_graham_essay.txt']).load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "ecaa6e44-7409-4c60-b2f8-d942aa5bf832",
      "metadata": {
        "id": "ecaa6e44-7409-4c60-b2f8-d942aa5bf832"
      },
      "outputs": [],
      "source": [
        "splitter = SentenceSplitter(\n",
        "    chunk_size=1024, # tokens\n",
        "    chunk_overlap=20,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "5885b616-34f7-4fa9-9018-65a097b83ca9",
      "metadata": {
        "id": "5885b616-34f7-4fa9-9018-65a097b83ca9"
      },
      "outputs": [],
      "source": [
        "nodes = splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "e2e9ecc3-d6f6-4010-be30-2c226646865b",
      "metadata": {
        "id": "e2e9ecc3-d6f6-4010-be30-2c226646865b",
        "outputId": "d03615d9-8ede-41b0-88d3-88b3bfc45421",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "62389bae-5e1a-4556-bf13-6e4655932e70",
      "metadata": {
        "scrolled": true,
        "id": "62389bae-5e1a-4556-bf13-6e4655932e70",
        "outputId": "6ab201e8-85c5-4cea-e40d-02a733652b59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id_': '92641104-d4ec-4a79-bbd2-5cdfd1034684',\n",
              " 'embedding': None,\n",
              " 'metadata': {'file_path': 'data/paul_graham_essay.txt',\n",
              "  'file_name': 'paul_graham_essay.txt',\n",
              "  'file_type': 'text/plain',\n",
              "  'file_size': 75042,\n",
              "  'creation_date': '2025-08-17',\n",
              "  'last_modified_date': '2025-08-17'},\n",
              " 'excluded_embed_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date'],\n",
              " 'excluded_llm_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date'],\n",
              " 'relationships': {'1': {'node_id': '1145946e-09a2-4f61-99bf-793aa74218af',\n",
              "   'node_type': '4',\n",
              "   'metadata': {'file_path': 'data/paul_graham_essay.txt',\n",
              "    'file_name': 'paul_graham_essay.txt',\n",
              "    'file_type': 'text/plain',\n",
              "    'file_size': 75042,\n",
              "    'creation_date': '2025-08-17',\n",
              "    'last_modified_date': '2025-08-17'},\n",
              "   'hash': 'b1bcab298313e0e4fdd59421d272e67878a68652ae8dc7d8f12d307aee99ad51',\n",
              "   'class_name': 'RelatedNodeInfo'},\n",
              "  '3': {'node_id': '56f7f329-309d-48fc-b3a7-2b86a854e2ca',\n",
              "   'node_type': '1',\n",
              "   'metadata': {},\n",
              "   'hash': '2b076feb45ef9318654997e47fb3b2f2185bcc7e39e0997045fcf755cb2ca6dc',\n",
              "   'class_name': 'RelatedNodeInfo'}},\n",
              " 'metadata_template': '{key}: {value}',\n",
              " 'metadata_separator': '\\n',\n",
              " 'text': 'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines ‚Äî CPU, disk drives, printer, card reader ‚Äî sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. [1]\\n\\nThe first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\\n\\nComputers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he\\'d write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\\n\\nThough I liked programming, I didn\\'t plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn\\'t much left for these supposed ultimate truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.\\n\\nI couldn\\'t have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\\n\\nAI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading The Moon is a Harsh Mistress, so I don\\'t know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most.',\n",
              " 'mimetype': 'text/plain',\n",
              " 'start_char_idx': 2,\n",
              " 'end_char_idx': 4320,\n",
              " 'metadata_seperator': '\\n',\n",
              " 'text_template': '{metadata_str}\\n\\n{content}',\n",
              " 'class_name': 'TextNode'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "nodes[0].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "207802f5-8294-4e9e-9e78-e31a72521dd3",
      "metadata": {
        "id": "207802f5-8294-4e9e-9e78-e31a72521dd3",
        "outputId": "97882b8a-3d28-494d-f268-a364136b6f7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What I Worked On\n",
            "\n",
            "February 2021\n",
            "\n",
            "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
            "\n",
            "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines ‚Äî CPU, disk drives, printer, card reader ‚Äî sitting up on a raised floor under bright fluorescent lights.\n",
            "\n",
            "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\n",
            "\n",
            "I was puzzled by the 1401. I couldn't figure out what to do with it. And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn't have any data stored on punched cards. The only other option was to do things that didn't rely on any input, like calculate approximations of pi, but I didn't know enough math to do anything interesting of that type. So I'm not surprised I can't remember any programs I wrote, because they can't have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn't. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager's expression made clear.\n",
            "\n",
            "With microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. [1]\n",
            "\n",
            "The first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\n",
            "\n",
            "Computers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he'd write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\n",
            "\n",
            "Though I liked programming, I didn't plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn't much left for these supposed ultimate truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.\n",
            "\n",
            "I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\n",
            "\n",
            "AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most.\n"
          ]
        }
      ],
      "source": [
        "print(nodes[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "051f027d-a779-4635-9b0c-5882f317bb97",
      "metadata": {
        "id": "051f027d-a779-4635-9b0c-5882f317bb97"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "47a5ee5d-6373-4dcf-993f-694e8b831b5a",
      "metadata": {
        "id": "47a5ee5d-6373-4dcf-993f-694e8b831b5a"
      },
      "source": [
        "## Sentence Window Node Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "64689daf-3d2f-4f46-9455-b829460ce478",
      "metadata": {
        "id": "64689daf-3d2f-4f46-9455-b829460ce478"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "from llama_index.core.node_parser import SentenceWindowNodeParser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "227f5b39-5607-4c59-aac9-0d1f9e845c73",
      "metadata": {
        "id": "227f5b39-5607-4c59-aac9-0d1f9e845c73"
      },
      "source": [
        "- SentenceSplitter, this parser splits text into individual sentences and also includes a window of surrounding sentences in the metadata of each node.\n",
        "- It is useful for building more context around each sentence.\n",
        "- During the querying process, that context will be fed into the LLM and allow for better responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "c50ff862-74f3-4b0f-ae78-6e30d0f07c67",
      "metadata": {
        "id": "c50ff862-74f3-4b0f-ae78-6e30d0f07c67"
      },
      "outputs": [],
      "source": [
        "sentence_window_parser = SentenceWindowNodeParser.from_defaults(\n",
        "    # how many sentences on either side to capture\n",
        "    window_size=3,\n",
        "\n",
        "    # the metadata key that holds the window of surrounding sentences\n",
        "    window_metadata_key=\"window\",\n",
        "\n",
        "    # the metadata key that holds the original sentence\n",
        "    original_text_metadata_key=\"original_sentence\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d383c4a3-696e-44a1-a2f5-462aef26eaf6",
      "metadata": {
        "id": "d383c4a3-696e-44a1-a2f5-462aef26eaf6"
      },
      "source": [
        "- `window_size`: This defines the number of sentences on each side to include in the window\n",
        "- `window_metadata_key`: This defines the metadata key for the window sentences\n",
        "- `original_text_metadata_key`: This defines the metadata key for the original sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "c4c57bad-2115-4fa6-92ad-44dd0e6e0954",
      "metadata": {
        "id": "c4c57bad-2115-4fa6-92ad-44dd0e6e0954"
      },
      "outputs": [],
      "source": [
        "nodes = sentence_window_parser.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "d3b8e935-0f20-4852-82b1-13f5ea4b69a8",
      "metadata": {
        "id": "d3b8e935-0f20-4852-82b1-13f5ea4b69a8",
        "outputId": "25e9e178-bdd2-4e93-a78d-d5cf0020774b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "757"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "36d9d605-6527-4dfa-98a0-1fe05b1c84be",
      "metadata": {
        "id": "36d9d605-6527-4dfa-98a0-1fe05b1c84be",
        "outputId": "90956e67-ef5b-455b-df67-d38d86aeeb15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id_': 'f66cec04-c62c-4105-b319-01d6311497c6',\n",
              " 'embedding': None,\n",
              " 'metadata': {'file_path': 'data/paul_graham_essay.txt',\n",
              "  'file_name': 'paul_graham_essay.txt',\n",
              "  'file_type': 'text/plain',\n",
              "  'file_size': 75042,\n",
              "  'creation_date': '2025-08-17',\n",
              "  'last_modified_date': '2025-08-17',\n",
              "  'window': \"\\n\\nWhat I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming.  I didn't write essays.  I wrote what beginning writers were supposed to write then, and probably still are: short stories.  My stories were awful. \",\n",
              "  'original_sentence': '\\n\\nWhat I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. '},\n",
              " 'excluded_embed_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date',\n",
              "  'window',\n",
              "  'original_sentence'],\n",
              " 'excluded_llm_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date',\n",
              "  'window',\n",
              "  'original_sentence'],\n",
              " 'relationships': {'1': {'node_id': '1145946e-09a2-4f61-99bf-793aa74218af',\n",
              "   'node_type': '4',\n",
              "   'metadata': {'file_path': 'data/paul_graham_essay.txt',\n",
              "    'file_name': 'paul_graham_essay.txt',\n",
              "    'file_type': 'text/plain',\n",
              "    'file_size': 75042,\n",
              "    'creation_date': '2025-08-17',\n",
              "    'last_modified_date': '2025-08-17'},\n",
              "   'hash': 'b1bcab298313e0e4fdd59421d272e67878a68652ae8dc7d8f12d307aee99ad51',\n",
              "   'class_name': 'RelatedNodeInfo'},\n",
              "  '3': {'node_id': '3de8b275-e539-4b83-a3e3-4e4d8ded7a72',\n",
              "   'node_type': '1',\n",
              "   'metadata': {'window': \"\\n\\nWhat I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming.  I didn't write essays.  I wrote what beginning writers were supposed to write then, and probably still are: short stories.  My stories were awful.  They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\n\",\n",
              "    'original_sentence': \"I didn't write essays. \"},\n",
              "   'hash': 'e84668afea15d54b192dd00caccceaa65afcdc4d03f275f90155628351b27fb1',\n",
              "   'class_name': 'RelatedNodeInfo'}},\n",
              " 'metadata_template': '{key}: {value}',\n",
              " 'metadata_separator': '\\n',\n",
              " 'text': '\\n\\nWhat I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. ',\n",
              " 'mimetype': 'text/plain',\n",
              " 'start_char_idx': 0,\n",
              " 'end_char_idx': 132,\n",
              " 'metadata_seperator': '\\n',\n",
              " 'text_template': '{metadata_str}\\n\\n{content}',\n",
              " 'class_name': 'TextNode'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "nodes[0].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "5679e7dc-c10f-45cb-b1f6-af3893ee533d",
      "metadata": {
        "id": "5679e7dc-c10f-45cb-b1f6-af3893ee533d",
        "outputId": "a4878150-8083-42d4-90db-381aad8298cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 \n",
            "\n",
            "What I Worked On\n",
            "\n",
            "February 2021\n",
            "\n",
            "Before college the two main things I worked on, outside of school, were writing and programming. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 I didn't write essays. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "2 I wrote what beginning writers were supposed to write then, and probably still are: short stories. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "3 My stories were awful. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "4 They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "5 The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" \n",
            "----------------------------------------------------------------------------------------------------\n",
            "6 This was in 9th grade, so I was 13 or 14. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "7 The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. \n",
            "----------------------------------------------------------------------------------------------------\n",
            "8 It was like a mini Bond villain's lair down there, with all these alien-looking machines ‚Äî CPU, disk drives, printer, card reader ‚Äî sitting up on a raised floor under bright fluorescent lights.\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "9 The language we used was an early version of Fortran. \n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    print(i, nodes[i].text)\n",
        "    print(\"-\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f36034b8-6004-4f38-aab1-a4d5bb5eea16",
      "metadata": {
        "id": "f36034b8-6004-4f38-aab1-a4d5bb5eea16"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2df3b63b-e258-4d45-adeb-24558dd46cd5",
      "metadata": {
        "id": "2df3b63b-e258-4d45-adeb-24558dd46cd5"
      },
      "source": [
        "## Semantic Splitter Node Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "62df7c7e-2753-40b5-9918-62de180787b0",
      "metadata": {
        "id": "62df7c7e-2753-40b5-9918-62de180787b0"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SemanticSplitterNodeParser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e35e163c-216b-46ff-beca-ca4dbe81dbb2",
      "metadata": {
        "id": "e35e163c-216b-46ff-beca-ca4dbe81dbb2"
      },
      "source": [
        "This parser requires embedding model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56fc188a-d0c4-4749-b32e-9ff41322218b",
      "metadata": {
        "id": "56fc188a-d0c4-4749-b32e-9ff41322218b"
      },
      "source": [
        "The Semantic Splitter Node Parser operates by initially dividing each sentence into segments called chunks. It then calculates the cosine dissimilarity between adjacent chunks, which measures their difference based on vector representations in semantic space. If this dissimilarity exceeds a predefined threshold, it suggests significant differences, and the chunks remain separate. Conversely, if the dissimilarity is below the threshold, indicating similarity, the chunks are concatenated into larger, unified chunks. This process helps ensure that each chunk represents a cohesive piece of information, improving the model's data processing efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "dcf1a243-37d5-4ade-becf-a8bb62578c53",
      "metadata": {
        "id": "dcf1a243-37d5-4ade-becf-a8bb62578c53"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "5496626f-b681-48f1-a0a3-a06e893c581a",
      "metadata": {
        "id": "5496626f-b681-48f1-a0a3-a06e893c581a"
      },
      "outputs": [],
      "source": [
        "embed_model = OpenAIEmbedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "b53e320d-2b36-4e8a-9643-d9670431b7b4",
      "metadata": {
        "id": "b53e320d-2b36-4e8a-9643-d9670431b7b4"
      },
      "outputs": [],
      "source": [
        "semantic_splitter = SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "a76ca5bd-e7aa-4a4c-a2b4-74e773c48f29",
      "metadata": {
        "id": "a76ca5bd-e7aa-4a4c-a2b4-74e773c48f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "0be99548-e4b6-4683-e392-bd9ff29fa07b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': \"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-244813688.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msemantic_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_nodes_from_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/node_parser/interface.py\u001b[0m in \u001b[0;36mget_nodes_from_documents\u001b[0;34m(self, documents, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mCBEventType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNODE_PARSING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mEventPayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOCUMENTS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         ) as event:\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_postprocess_parsed_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_id_to_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index_instrumentation/dispatcher.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;31m# If the result is a Future, wrap it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/node_parser/text/semantic_splitter.py\u001b[0m in \u001b[0;36m_parse_nodes\u001b[0;34m(self, nodes, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes_with_progress\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_semantic_nodes_from_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mall_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/node_parser/text/semantic_splitter.py\u001b[0m in \u001b[0;36mbuild_semantic_nodes_from_documents\u001b[0;34m(self, documents, show_progress)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_sentence_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             combined_sentence_embeddings = self.embed_model.get_text_embedding_batch(\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"combined_sentence\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index_instrumentation/dispatcher.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;31m# If the result is a Future, wrap it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/base/embeddings/base.py\u001b[0m in \u001b[0;36mget_text_embedding_batch\u001b[0;34m(self, texts, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m                 ) as event:\n\u001b[1;32m    472\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m                         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_text_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_text_embeddings_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/embeddings/openai/base.py\u001b[0m in \u001b[0;36m_get_text_embeddings\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_retryable_get_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_aget_text_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_post_retry_check_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RetryCallState\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_run_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/embeddings/openai/base.py\u001b[0m in \u001b[0;36m_retryable_get_embeddings\u001b[0;34m()\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mretry_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_retryable_get_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             return get_embeddings(\n\u001b[0m\u001b[1;32m    466\u001b[0m                 \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/embeddings/openai/base.py\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(client, list_of_text, engine, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlist_of_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist_of_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/embeddings.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;34m\"/embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_create_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingCreateParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': \"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
          ]
        }
      ],
      "source": [
        "nodes = semantic_splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1454e85-e5d9-4fd9-96b7-8c7118f68468",
      "metadata": {
        "id": "a1454e85-e5d9-4fd9-96b7-8c7118f68468"
      },
      "outputs": [],
      "source": [
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5731d35e-50b4-4441-a48a-45f1bf6740ad",
      "metadata": {
        "scrolled": true,
        "id": "5731d35e-50b4-4441-a48a-45f1bf6740ad"
      },
      "outputs": [],
      "source": [
        "nodes[0].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee873dc9-e9c9-48a0-a69f-45a068544a47",
      "metadata": {
        "id": "ee873dc9-e9c9-48a0-a69f-45a068544a47"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    print(i, nodes[i].text)\n",
        "    print(\"_\"*150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8aecb4f-66ab-48ed-9c93-773432fba5cb",
      "metadata": {
        "id": "f8aecb4f-66ab-48ed-9c93-773432fba5cb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "02bba916-6971-46b0-b8e2-c0f4c3781566",
      "metadata": {
        "id": "02bba916-6971-46b0-b8e2-c0f4c3781566"
      },
      "source": [
        "## TokenTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "7bc177fe-a0bb-4b8d-b0b0-b5b04da9c84c",
      "metadata": {
        "id": "7bc177fe-a0bb-4b8d-b0b0-b5b04da9c84c"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import TokenTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "06c0fb84-ed91-4390-9693-338287148e17",
      "metadata": {
        "id": "06c0fb84-ed91-4390-9693-338287148e17"
      },
      "outputs": [],
      "source": [
        "token_text_splitter = TokenTextSplitter(\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=20,\n",
        "    separator=\" \",\n",
        "    backup_separators = [\".\", \"!\", \"?\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db290819-76de-46e0-b10f-b942980c22d4",
      "metadata": {
        "id": "db290819-76de-46e0-b10f-b942980c22d4"
      },
      "source": [
        "- `chunk_size`: This sets the maximum number of tokens for each chunk\n",
        "- `chunk_overlap`: This defines the overlap in tokens between consecutive chunks\n",
        "- `separator`: This is used to determine the primary token boundary\n",
        "- `backup_separators`: These can be used for additional splitting points if the primary separator doesn‚Äôt split the text sufficiently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "ee6d4420-cf00-478d-b25c-4cc5c07ba625",
      "metadata": {
        "id": "ee6d4420-cf00-478d-b25c-4cc5c07ba625"
      },
      "outputs": [],
      "source": [
        "nodes = token_text_splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "ac4414a9-0ef8-48b2-9820-7af4af2955bf",
      "metadata": {
        "id": "ac4414a9-0ef8-48b2-9820-7af4af2955bf",
        "outputId": "855cdb5c-a4b3-4db5-b99e-7fd428ddfa02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "5e6671df-ce48-4784-9348-623680fa28c9",
      "metadata": {
        "id": "5e6671df-ce48-4784-9348-623680fa28c9",
        "outputId": "46349fd9-027b-4def-f27c-f1c608a6c7f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id_': '66fbe196-d8df-42a8-aafc-4944368b9de2',\n",
              " 'embedding': None,\n",
              " 'metadata': {'file_path': 'data/paul_graham_essay.txt',\n",
              "  'file_name': 'paul_graham_essay.txt',\n",
              "  'file_type': 'text/plain',\n",
              "  'file_size': 75042,\n",
              "  'creation_date': '2025-08-17',\n",
              "  'last_modified_date': '2025-08-17'},\n",
              " 'excluded_embed_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date'],\n",
              " 'excluded_llm_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date'],\n",
              " 'relationships': {'1': {'node_id': '1145946e-09a2-4f61-99bf-793aa74218af',\n",
              "   'node_type': '4',\n",
              "   'metadata': {'file_path': 'data/paul_graham_essay.txt',\n",
              "    'file_name': 'paul_graham_essay.txt',\n",
              "    'file_type': 'text/plain',\n",
              "    'file_size': 75042,\n",
              "    'creation_date': '2025-08-17',\n",
              "    'last_modified_date': '2025-08-17'},\n",
              "   'hash': 'b1bcab298313e0e4fdd59421d272e67878a68652ae8dc7d8f12d307aee99ad51',\n",
              "   'class_name': 'RelatedNodeInfo'},\n",
              "  '3': {'node_id': '856ce7d9-f84d-4475-a96a-4e6cdbb2b17a',\n",
              "   'node_type': '1',\n",
              "   'metadata': {},\n",
              "   'hash': '629002462c14a725076bc0ebc465c1f1ae284c0d393906be5e36283a92876716',\n",
              "   'class_name': 'RelatedNodeInfo'}},\n",
              " 'metadata_template': '{key}: {value}',\n",
              " 'metadata_separator': '\\n',\n",
              " 'text': 'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines ‚Äî CPU, disk drives, printer, card reader ‚Äî sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. [1]\\n\\nThe first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\\n\\nComputers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he\\'d write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\\n\\nThough I liked programming, I didn\\'t plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn\\'t much left for these supposed ultimate truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.\\n\\nI couldn\\'t have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\\n\\nAI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading The Moon is a Harsh Mistress, so I don\\'t know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.\\n\\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp,',\n",
              " 'mimetype': 'text/plain',\n",
              " 'start_char_idx': 2,\n",
              " 'end_char_idx': 4508,\n",
              " 'metadata_seperator': '\\n',\n",
              " 'text_template': '{metadata_str}\\n\\n{content}',\n",
              " 'class_name': 'TextNode'}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "nodes[0].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "725e11d8-7aa3-4a25-8b4e-a6821ecfa76d",
      "metadata": {
        "id": "725e11d8-7aa3-4a25-8b4e-a6821ecfa76d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "b3620b21-9884-4790-8840-5c7dce46f2a8",
      "metadata": {
        "id": "b3620b21-9884-4790-8840-5c7dce46f2a8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bf65fbe2-fea7-428a-9360-15af2758de62",
      "metadata": {
        "id": "bf65fbe2-fea7-428a-9360-15af2758de62"
      },
      "source": [
        "# Relation-Based Node Parsers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7808d558-0806-4359-8feb-fc3a85d2519d",
      "metadata": {
        "id": "7808d558-0806-4359-8feb-fc3a85d2519d"
      },
      "source": [
        "## Hierarchical Node Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "59001c5d-e853-45b6-9c97-c756eb422be4",
      "metadata": {
        "id": "59001c5d-e853-45b6-9c97-c756eb422be4"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import HierarchicalNodeParser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "165468c8-107e-47ab-9257-d41e1ad42a60",
      "metadata": {
        "id": "165468c8-107e-47ab-9257-d41e1ad42a60"
      },
      "source": [
        "This parser organizes the nodes into hierarchies across multiple levels.\n",
        "It will generate a hierarchy of nodes, starting with top-level nodes with larger section sizes, down to child nodes with smaller section sizes, where each child node has a parent node with a larger section size.\n",
        "By default, the parser uses SentenceSplitter to chunk text. The node hierarchy looks like this:\n",
        "\n",
        "- Level 1: Section size 2,048\n",
        "- Level 2: Section size 512\n",
        "- Level 3: Section size 128\n",
        "\n",
        "The top-level nodes, with larger sections, can provide high-level summaries, while the lower nodes can allow for a more detailed analysis of text sections.\n",
        "In this way, the different node levels can be used to adjust the accuracy and depth of search results, allowing users to find information at different granularity levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "0988ab22-6bbb-4db0-abc1-2038eae626c2",
      "metadata": {
        "id": "0988ab22-6bbb-4db0-abc1-2038eae626c2"
      },
      "outputs": [],
      "source": [
        "hierarchical_node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=[2048, 512, 128])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "53e1fcb1-a59c-4262-a2e8-d0521db5abad",
      "metadata": {
        "id": "53e1fcb1-a59c-4262-a2e8-d0521db5abad"
      },
      "outputs": [],
      "source": [
        "nodes = hierarchical_node_parser.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "de116c68-0eba-4ba0-a780-6bef900dfef7",
      "metadata": {
        "id": "de116c68-0eba-4ba0-a780-6bef900dfef7",
        "outputId": "215a8809-1287-46ef-bd44-28c5a1cbc536",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "243"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "40211558-6c53-4df9-b3eb-04720c42258b",
      "metadata": {
        "scrolled": true,
        "id": "40211558-6c53-4df9-b3eb-04720c42258b",
        "outputId": "c334abea-9da2-402f-b54c-34dccd739dd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id_': '2523ca1f-92b3-4d9b-aa4e-3429d2a5c215',\n",
              " 'embedding': None,\n",
              " 'metadata': {'file_path': 'data/paul_graham_essay.txt',\n",
              "  'file_name': 'paul_graham_essay.txt',\n",
              "  'file_type': 'text/plain',\n",
              "  'file_size': 75042,\n",
              "  'creation_date': '2025-08-17',\n",
              "  'last_modified_date': '2025-08-17'},\n",
              " 'excluded_embed_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date'],\n",
              " 'excluded_llm_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date'],\n",
              " 'relationships': {'1': {'node_id': '1145946e-09a2-4f61-99bf-793aa74218af',\n",
              "   'node_type': '4',\n",
              "   'metadata': {'file_path': 'data/paul_graham_essay.txt',\n",
              "    'file_name': 'paul_graham_essay.txt',\n",
              "    'file_type': 'text/plain',\n",
              "    'file_size': 75042,\n",
              "    'creation_date': '2025-08-17',\n",
              "    'last_modified_date': '2025-08-17'},\n",
              "   'hash': 'b1bcab298313e0e4fdd59421d272e67878a68652ae8dc7d8f12d307aee99ad51',\n",
              "   'class_name': 'RelatedNodeInfo'},\n",
              "  '3': {'node_id': '68b9c594-0cc8-4762-a16e-28b7d420b8d7',\n",
              "   'node_type': '1',\n",
              "   'metadata': {},\n",
              "   'hash': '38d894da37650955c93594af977c8f6ebeb73cac827eba4b87e480281d6da170',\n",
              "   'class_name': 'RelatedNodeInfo'},\n",
              "  '5': [{'node_id': '68fd3a1c-309c-4f3d-9608-809ddf36bf13',\n",
              "    'node_type': '1',\n",
              "    'metadata': {'file_path': 'data/paul_graham_essay.txt',\n",
              "     'file_name': 'paul_graham_essay.txt',\n",
              "     'file_type': 'text/plain',\n",
              "     'file_size': 75042,\n",
              "     'creation_date': '2025-08-17',\n",
              "     'last_modified_date': '2025-08-17'},\n",
              "    'hash': '87e0569f1c0e539ae251c2d0a0e08befd57b3f2562243f5309476b54ccde5c5e',\n",
              "    'class_name': 'RelatedNodeInfo'},\n",
              "   {'node_id': '95bce45e-f04f-407f-b877-d79c9b1f54ee',\n",
              "    'node_type': '1',\n",
              "    'metadata': {'file_path': 'data/paul_graham_essay.txt',\n",
              "     'file_name': 'paul_graham_essay.txt',\n",
              "     'file_type': 'text/plain',\n",
              "     'file_size': 75042,\n",
              "     'creation_date': '2025-08-17',\n",
              "     'last_modified_date': '2025-08-17'},\n",
              "    'hash': 'dadc5faf06ed782832be3ffe3f0e7601dd9e478f1a0861453a595b11c61ad26d',\n",
              "    'class_name': 'RelatedNodeInfo'},\n",
              "   {'node_id': '1e2b73cd-69b8-42a4-988a-4372b924b98c',\n",
              "    'node_type': '1',\n",
              "    'metadata': {'file_path': 'data/paul_graham_essay.txt',\n",
              "     'file_name': 'paul_graham_essay.txt',\n",
              "     'file_type': 'text/plain',\n",
              "     'file_size': 75042,\n",
              "     'creation_date': '2025-08-17',\n",
              "     'last_modified_date': '2025-08-17'},\n",
              "    'hash': '31a28158fc0b15bd0c646603cb63e91e53834563d5ecc2374e90d5220092e25a',\n",
              "    'class_name': 'RelatedNodeInfo'},\n",
              "   {'node_id': '565f5f36-ccef-4e67-83d0-9ce7bd79f3d9',\n",
              "    'node_type': '1',\n",
              "    'metadata': {'file_path': 'data/paul_graham_essay.txt',\n",
              "     'file_name': 'paul_graham_essay.txt',\n",
              "     'file_type': 'text/plain',\n",
              "     'file_size': 75042,\n",
              "     'creation_date': '2025-08-17',\n",
              "     'last_modified_date': '2025-08-17'},\n",
              "    'hash': '770fab30b5af6868dcafc2368496012446213b2f47045c2d583fea24ef507a51',\n",
              "    'class_name': 'RelatedNodeInfo'},\n",
              "   {'node_id': 'b340b792-1bf3-4c1b-94ab-cad2ae398e2f',\n",
              "    'node_type': '1',\n",
              "    'metadata': {'file_path': 'data/paul_graham_essay.txt',\n",
              "     'file_name': 'paul_graham_essay.txt',\n",
              "     'file_type': 'text/plain',\n",
              "     'file_size': 75042,\n",
              "     'creation_date': '2025-08-17',\n",
              "     'last_modified_date': '2025-08-17'},\n",
              "    'hash': '308618609a610037115f785ceb25f68b25083f8ba160c7aa644b8d39c705bcc6',\n",
              "    'class_name': 'RelatedNodeInfo'}]},\n",
              " 'metadata_template': '{key}: {value}',\n",
              " 'metadata_separator': '\\n',\n",
              " 'text': 'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines ‚Äî CPU, disk drives, printer, card reader ‚Äî sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. [1]\\n\\nThe first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\\n\\nComputers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he\\'d write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\\n\\nThough I liked programming, I didn\\'t plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn\\'t much left for these supposed ultimate truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.\\n\\nI couldn\\'t have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\\n\\nAI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading The Moon is a Harsh Mistress, so I don\\'t know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.\\n\\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of AI. The commonly used programming languages then were pretty primitive, and programmers\\' ideas correspondingly so. The default language at Cornell was a Pascal-like language called PL/I, and the situation was similar elsewhere. Learning Lisp expanded my concept of a program so fast that it was years before I started to have a sense of where the new limits were. This was more like it; this was what I had expected college to do. It wasn\\'t happening in a class, like it was supposed to, but that was ok. For the next couple years I was on a roll. I knew what I was going to do.\\n\\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love working on that program. It was a pleasing bit of code, but what made it even more exciting was my belief ‚Äî hard to imagine now, but not unique in 1985 ‚Äî that it was already climbing the lower slopes of intelligence.\\n\\nI had gotten into a program at Cornell that didn\\'t make you choose a major. You could take whatever classes you liked, and choose whatever you liked to put on your degree. I of course chose \"Artificial Intelligence.\" When I got the actual physical diploma, I was dismayed to find that the quotes had been included, which made them read as scare-quotes. At the time this bothered me, but now it seems amusingly accurate, for reasons I was about to discover.\\n\\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the time, and Harvard, which I\\'d visited because Rich Draves went there, and was also home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU clone. Only Harvard accepted me, so that was where I went.\\n\\nI don\\'t remember the moment it happened, or if there even was a specific moment, but during the first year of grad school I realized that AI, as practiced at the time, was a hoax. By which I mean the sort of AI in which a program that\\'s told \"the dog is sitting on the chair\" translates this into some formal representation and adds it to the list of things it knows.\\n\\nWhat these programs really showed was that there\\'s a subset of natural language that\\'s a formal language. But a very proper subset. It was clear that there was an unbridgeable gap between what they could do and actually understanding natural language. It was not, in fact, simply a matter of teaching SHRDLU more words. That whole way of doing AI, with explicit data structures representing concepts, was not going to work. Its brokenness did, as so often happens, generate a lot of opportunities to write papers about various band-aids that could be applied to it, but it was never going to get us Mike.\\n\\nSo I looked around to see what I could salvage from the wreckage of my plans, and there was Lisp. I knew from experience that Lisp was interesting for its own sake and not just for its association with AI, even though that was the main reason people cared about it at the time. So I decided to focus on Lisp. In fact, I decided to write a book about Lisp hacking. It\\'s scary to think how little I knew about Lisp hacking when I started writing that book. But there\\'s nothing like writing a book about something to help you learn it. The book, On Lisp, wasn\\'t published till 1993, but I wrote much of it in grad school.\\n\\nComputer Science is an uneasy alliance between two halves, theory and systems. The theory people prove things, and the systems people build things. I wanted to build things. I had plenty of respect for theory ‚Äî indeed, a sneaking suspicion that it was the more admirable of the two halves ‚Äî but building things seemed so much more exciting.\\n\\nThe problem with systems work, though, was that it didn\\'t last. Any program you wrote today, no matter how good, would be obsolete in a couple decades at best. People might mention your software in footnotes, but no one would actually use it. And indeed, it would seem very feeble work. Only people with a sense of the history of the field would even realize that, in its time, it had been good.\\n\\nThere were some surplus Xerox Dandelions floating around the computer lab at one point. Anyone who wanted one to play around with could have one. I was briefly tempted, but they were so slow by present standards; what was the point? No one else wanted one either, so off they went. That was what happened to systems work.',\n",
              " 'mimetype': 'text/plain',\n",
              " 'start_char_idx': 2,\n",
              " 'end_char_idx': 8846,\n",
              " 'metadata_seperator': '\\n',\n",
              " 'text_template': '{metadata_str}\\n\\n{content}',\n",
              " 'class_name': 'TextNode'}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "nodes[0].to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e7929c-b009-4d42-93e8-77c13402c415",
      "metadata": {
        "id": "70e7929c-b009-4d42-93e8-77c13402c415"
      },
      "source": [
        "# Node parsers vs text splitters\n",
        "\n",
        "To simplify, a node parser is a more sophisticated mechanism than a simple splitter. While both serve the same basic function and operate at different levels of complexity, they differ in their implementations.\n",
        "\n",
        "- Text splitters such as SentenceSplitter can divide long flat texts into nodes, based on certain rules or limitations, such as chunk_size or chunk_overlap. The nodes could represent lines, paragraphs, or sentences, and may also include additional metadata or links to the original document.\n",
        "\n",
        "- Node parsers are more sophisticated and can involve additional data processing logic. Beyond simply dividing text into nodes, they can perform extra tasks, such as analyzing the structure of HTML or JSON files and producing nodes enriched with contextual information."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HhKRXr_GdXVf"
      },
      "id": "HhKRXr_GdXVf",
      "execution_count": 58,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}